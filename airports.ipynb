{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d049090a-0adf-4d4b-9f02-518af8846ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: folium in /users/samuelking/.local/lib/python3.11/site-packages (0.16.0)\n",
      "Requirement already satisfied: xyzservices in /usr/local/lib/python3.11/dist-packages (from folium) (2024.6.0)\n",
      "Requirement already satisfied: branca>=0.6.0 in /users/samuelking/.local/lib/python3.11/site-packages (from folium) (0.7.2)\n",
      "Requirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.11/dist-packages (from folium) (3.1.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from folium) (1.26.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from folium) (2.32.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.9->folium) (2.1.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->folium) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->folium) (2024.6.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->folium) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->folium) (3.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd2ebc93-f54f-43ae-884f-9ecec3f5b184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- Import packages --------------\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely import Point\n",
    "import folium\n",
    "import re\n",
    "import numpy as np\n",
    "%config InlineBackend.figure_format ='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f34b6ae3-5021-48ca-8cb8-83f70f54c84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- Entering US county data ------------------\n",
    "# Read in US counties shapefiles\n",
    "us_counties = gpd.read_file('~/Human Trafficking Project/Data/usShapeFiles/cb_2018_us_county_500k.shp')\n",
    "\n",
    "# Combine the county name and state FIPS to give each county a unique name\n",
    "us_counties['name_and_fips'] = us_counties[[\"NAME\",\"STATEFP\"]].agg(\" \".join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5f268d-8234-4964-9cf4-389b8c17fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Counting US airports by county -----------------\n",
    "# Read in csv containing all U.S. airports\n",
    "us_airports = pd.read_csv('~/Human Trafficking Project/Data/us-airports.csv')\n",
    "\n",
    "# Limit the dataset to only medium and large airports.\n",
    "us_airports['airport'] = us_airports['type'].isin(['large_airport', 'medium_airport'])\n",
    "us_airports_v2 = us_airports[us_airports['airport']]\n",
    "\n",
    "# Create a geometry column out to the longitude and latitude column\n",
    "us_airports_v2['coord'] = us_airports_v2.apply(lambda row: Point(row['longitude_deg'], row['latitude_deg']), axis=1)\n",
    "\n",
    "# Create a GeoDataFrame using the geometry column\n",
    "us_airports_gdf = gpd.GeoDataFrame(us_airports_v2, geometry=us_airports_v2['coord'], crs='EPSG:4326')\n",
    "\n",
    "#a function to count the number of points within polygons in a GeoDataFrame\n",
    "def count_points_in_polygons(points_gdf, polygons_gdf):\n",
    "    \"\"\"\n",
    "    Count how many points fall into each polygon in polygons_gdf.\n",
    "\n",
    "    Parameters:\n",
    "    - points_gdf: GeoDataFrame containing points.\n",
    "    - polygons_gdf: GeoDataFrame containing polygons.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with polygon IDs and corresponding point counts.\n",
    "    \"\"\"\n",
    "    # Ensure points and polygons have the same CRS\n",
    "    points_gdf = points_gdf.to_crs(polygons_gdf.crs)\n",
    "\n",
    "    # Perform spatial join to count points within each polygon\n",
    "    points_in_polygons = gpd.sjoin(points_gdf, polygons_gdf, op='within')\n",
    "\n",
    "    # Group by polygon and count points\n",
    "    point_counts = points_in_polygons.groupby('name_and_fips').size().reset_index(name='point_count')\n",
    "\n",
    "    return point_counts\n",
    "\n",
    "#Counting the number of airports per county\n",
    "airport_count = count_points_in_polygons(us_airports_gdf, us_counties)\n",
    "\n",
    "# Merging the airport counts back onto the counties GeoDataFrame\n",
    "airport_counts= pd.merge(airport_count, us_counties, left_on=\"name_and_fips\", right_on=\"name_and_fips\", how='left')\n",
    "airport_count_gdf = gpd.GeoDataFrame(airport_counts, geometry='geometry', crs='EPSG:4326') \n",
    "\n",
    "# Create a list of states and territory FIPS to remove for mapping\n",
    "exclude_statefps = ['02', '15', '66', '72', '60','61','62','63', '64', '66','68', '69', '70', '74','78', '81','84','86','67','89','71','76','95','79']\n",
    "\n",
    "# Filtering the DataFrame\n",
    "lower_48 = us_counties[~us_counties['STATEFP'].isin(exclude_statefps)]\n",
    "\n",
    "# Plot the GeoDataFrame after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdbceef-fb81-4561-ab37-afda24cd0978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- Counting major roadways by US county -------------\n",
    "# Read in Census shapefile for major US roads\n",
    "roads = gpd.read_file('~/Human Trafficking Project/Data/USRoads/tl_2021_us_primaryroads.shp')\n",
    "\n",
    "# Limit the dataset to only those that are interstate\n",
    "roads['interstate']=roads['RTTYP'].isin(['I'])\n",
    "roadsv2 = roads[roads['interstate']]\n",
    "\n",
    "# Define a fucntion to count linestrings in a polygon\n",
    "def count_linestrings_in_polygons(linestrings_gdf, polygons_gdf):\n",
    "    \"\"\"\n",
    "    Count how many linestrings fall into each polygon in polygons_gdf.\n",
    "\n",
    "    Parameters:\n",
    "    - linestrings_gdf: GeoDataFrame containing linestrings.\n",
    "    - polygons_gdf: GeoDataFrame containing polygons.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with polygon IDs and corresponding linestring counts.\n",
    "    \"\"\"\n",
    "    # Ensure linestrings and polygons have the same CRS\n",
    "    linestrings_gdf = linestrings_gdf.to_crs(polygons_gdf.crs)\n",
    "    \n",
    "    # Perform spatial join to find linestrings within each polygon\n",
    "    linestrings_in_polygons = gpd.sjoin(linestrings_gdf, polygons_gdf, op='intersects')\n",
    "    \n",
    "    # Group by polygon and count linestrings\n",
    "    linestring_counts = linestrings_in_polygons.groupby('name_and_fips').size().reset_index(name='linestring_count')\n",
    "\n",
    "    return linestring_counts\n",
    "\n",
    "#Counting the number of roadways per county \n",
    "highway_count = count_linestrings_in_polygons(roadsv2, us_counties)\n",
    "\n",
    "# Merge the highway counts by county onto the continental dataframe to match\n",
    "highway_counts= pd.merge(highway_count, lower_48, left_on=\"name_and_fips\", right_on=\"name_and_fips\", how='left')\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame\n",
    "highway_count_gdf = gpd.GeoDataFrame(highway_counts, geometry='geometry', crs='EPSG:4326') \n",
    "\n",
    "#ax = lower_48.plot(color='grey')\n",
    "#highway_count_gdf.plot(column='linestring_count', ax=ax, figsize=(50,5))\n",
    "\n",
    "# # Export the highway count geodataframe as a csv\n",
    "# highway_count_gdf.to_csv('highway_count.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758dd595-eaca-439a-8281-8eb715b7a4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ Finding percentage reservation land by county -------------\n",
    "# Read in Census shapfile of all reservation land in the US\n",
    "reservations = gpd.read_file('~/Human Trafficking Project/Data/Reservationsv2/tl_2018_us_aiannh.shp')\n",
    "\n",
    "# Check the CRS of the shapefile\n",
    "reservations.crs\n",
    "\n",
    "# Ensure both GeoDataFrames have the same CRS if not already\n",
    "us_counties = us_counties.to_crs('EPSG:4326')  # WGS84\n",
    "reservations = reservations.to_crs('EPSG:4326')  # WGS84\n",
    "\n",
    "# ---------- Creating a folium map to confirm the accuracy of the shapefiles\n",
    "# Create a Folium map centered around the mean of all coordinates\n",
    "map_center = [us_counties['geometry'].centroid.y.mean(), us_counties['geometry'].centroid.x.mean()]\n",
    "mymap = folium.Map(location=map_center, zoom_start=4)\n",
    "\n",
    "# Add US counties GeoDataFrame as a GeoJSON layer to the map\n",
    "folium.GeoJson(\n",
    "    us_counties,\n",
    "    name='US Counties',\n",
    "    tooltip=folium.GeoJsonTooltip(fields=['NAME']),\n",
    ").add_to(mymap)\n",
    "\n",
    "# Add reservations GeoDataFrame as a GeoJSON layer to the map\n",
    "folium.GeoJson(\n",
    "    reservations,\n",
    "    name='Reservations',\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': 'black',\n",
    "        'color': 'black',\n",
    "        'weight': 1,\n",
    "        'fillOpacity': 0.7,\n",
    "    },\n",
    "    tooltip=folium.GeoJsonTooltip(fields=['NAME']),\n",
    ").add_to(mymap)\n",
    "\n",
    "# Add Layer Control to the map (allows switching layers on/off)\n",
    "folium.LayerControl().add_to(mymap)\n",
    "\n",
    "# Display the map\n",
    "#mymap\n",
    "\n",
    "\n",
    "# TODO some of the code got deleted in this section, which corrected the crs of both dataframes and accurately found the overlay.\n",
    "# As it wasn't critical to our analysis, I didn't finnish the work to rewrite the code. The code will run, but areas are incorrect.\n",
    "# us_counties = us_counties.to_crs('EPSG:3857')  # WGS84\n",
    "reservations.to_crs('EPSG:3857')  # WGS84\n",
    "us_counties.to_crs('EPSG:3857')\n",
    "\n",
    "# Overlay the reservations on top of the counties, keeping only the intersections\n",
    "res_intersection = reservations.overlay(us_counties, how='intersection')\n",
    "\n",
    "# Find the area of the resulting intersection polygons\n",
    "res_intersection['area_sqm']=res_intersection.geometry.area\n",
    "\n",
    "# Filtering the DataFrame to only the lower 48\n",
    "res_intersectionv2 = res_intersection[~res_intersection['STATEFP'].isin(exclude_statefps)]\n",
    "\n",
    "# Ensure GeoDataFrame is in a projected CRS for accurate area calculation (e.g., EPSG:3857)\n",
    "us_counties_3857 = us_counties.to_crs('EPSG:3857')\n",
    "\n",
    "# Calculate area of each polygon in square meters\n",
    "us_counties_3857['county_area_sqm'] = us_counties_3857.geometry.area\n",
    "\n",
    "overlaps = pd.merge(us_counties_3857, res_intersection, left_on=\"name_and_fips\", right_on=\"name_and_fips\", how='left')\n",
    "\n",
    "overlaps['percent_res'] = (overlaps['area_sqm'] / overlaps['county_area_sqm']) * 100\n",
    "\n",
    "overlaps.tail(30)\n",
    "\n",
    "# Remove rows with NaN in the 'percent_res' column\n",
    "overlaps.dropna(subset=['percent_res'], inplace=True)\n",
    "\n",
    "overlaps_48=overlaps[~overlaps['name_and_fips'].isin(exclude_statefps)]\n",
    "\n",
    "# ax=lower_48.plot(color='grey')\n",
    "# overlaps_48.plot(ax=ax, column='percent_res')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cbc8ca-dc6b-4fa2-b1e0-e2e66be237ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Beginning an analysis of human trafficking and percentage reservation ------------\n",
    "# Read in the cleaned human trafficking data\n",
    "htdata = pd.read_csv('~/Human Trafficking Project/Products/CleanedNIBRS2022v2.csv')\n",
    "\n",
    "# Drop counties that don't contain human trafficking incidents\n",
    "htdata.dropna(subset=['count'], inplace = True)\n",
    "\n",
    "# Convert FIPS codes and state names to strings, then aggregating them to create unique names for each county\n",
    "htdata['STATEFP'] = htdata['STATEFP'].astype(str)\n",
    "htdata['NAME'] = htdata['NAME'].astype(str)\n",
    "htdata['name_and_fips'] = htdata[[\"NAME\",\"STATEFP\"]].agg(\" \".join, axis=1)\n",
    "\n",
    "# Drop the old index column\n",
    "htdata.drop(columns=['Unnamed: 0.1'], inplace=True)\n",
    "\n",
    "# Merge the reservation/county overlap data with the human trafficking data\n",
    "reshtdata= pd.merge(overlaps, htdata, left_on=\"name_and_fips\", right_on=\"name_and_fips\", how='left')\n",
    "reshtdata.dropna(subset=['count'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90da6815-fe4a-4142-9e0a-b1325cf04f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
